{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Ciphertext Decryption Breakdown/Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text as tf_text\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from model_util.ciphers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cipher(words):\n",
    "    inp = int(input('What numeric key would you like to use for railfence ciphering? '))\n",
    "    func = np.vectorize(railfence)\n",
    "    return func(words, inp)\n",
    "\n",
    "def one_hot_encoding(word, uniques):\n",
    "    N = word.shape[0]\n",
    "    enc = np.zeros((N, uniques.shape[0]))\n",
    "    for i in range(N):\n",
    "        enc[i] = (word[i] == uniques)\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load words\n",
    "words = np.loadtxt('words_alpha.txt', dtype = str)[:150000]\n",
    "\n",
    "M = words.shape[0]\n",
    "MAX_WORD_LENGTH = len(max(words, key = len))\n",
    "\n",
    "# Shuffle word list\n",
    "np.random.seed(100)\n",
    "idx = np.random.permutation(M)\n",
    "words = words[idx]\n",
    "# Encipher words\n",
    "words_enc = cipher(words)\n",
    "\n",
    "# Tokenize words for RNN input\n",
    "tokenizer = tf_text.UnicodeCharTokenizer()\n",
    "X_tokens = tokenizer.tokenize(words_enc).to_list()\n",
    "y_tokens = tokenizer.tokenize(words).to_list()\n",
    "\n",
    "# Pad tokens so inputs are all the same size\n",
    "X_pad = pad_sequences(X_tokens, maxlen = MAX_WORD_LENGTH, padding = 'post', truncating = 'post')\n",
    "y_pad = pad_sequences(y_tokens, maxlen = MAX_WORD_LENGTH, padding = 'post', truncating = 'post')\n",
    "\n",
    "uniques = np.unique(y_pad)\n",
    "NUM_UNIQUES = uniques.shape[0]\n",
    "\n",
    "# Build dataset and labels\n",
    "X = np.zeros((M, MAX_WORD_LENGTH, NUM_UNIQUES))\n",
    "y = np.zeros((M, MAX_WORD_LENGTH, NUM_UNIQUES))\n",
    "for i in range(M):\n",
    "    X[i] = one_hot_encoding(X_pad[i], uniques)\n",
    "    y[i] = one_hot_encoding(y_pad[i], uniques)\n",
    "\n",
    "# Construct test and training sets\n",
    "split = int(M * 0.8)\n",
    "train_x, test_x = X[:split], X[split:]\n",
    "train_y, test_y = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 150000\n",
      "Max word length: 28\n",
      "Some words: ['imsonic' 'arminianism' 'duckers']\n",
      "Some encryptions: ['imscoin' 'airnsmamiin' 'ducskre']\n",
      "\n",
      "Some tokens:\n",
      "[[105 109 115 111 110 105  99   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [ 97 114 109 105 110 105  97 110 105 115 109   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [100 117  99 107 101 114 115   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "\n",
      "Number of unique tokens: 27\n",
      "All unique tokens:\n",
      "[  0  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113\n",
      " 114 115 116 117 118 119 120 121 122]\n",
      "\n",
      "Shape of one-hot encoding (by character): (150000, 28, 27)\n",
      "Example:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of words: {M}')\n",
    "print(f'Max word length: {MAX_WORD_LENGTH}')\n",
    "print(f'Some words: {words[:3]}')\n",
    "print(f'Some encryptions: {words_enc[:3]}', end = '\\n\\n')\n",
    "print(f'Some tokens:\\n{y_pad[:3]}', end = '\\n\\n')\n",
    "print(f'Number of unique tokens: {NUM_UNIQUES}')\n",
    "print(f'All unique tokens:\\n{uniques}', end = '\\n\\n')\n",
    "print(f'Shape of one-hot encoding (by character): {X.shape}')\n",
    "print(f'Example:\\n{X[0][:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model: Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X, y, learning_rate = 0.01, epochs = 3, validation_split = 0.3, units = 128, batch_size = 36):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(units, return_sequences = True), input_shape = (MAX_WORD_LENGTH, NUM_UNIQUES)),\n",
    "        Bidirectional(LSTM(32, return_sequences = True)),\n",
    "        Dense(NUM_UNIQUES, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer = Adam(learning_rate = learning_rate), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    history = model.fit(X, y, epochs = epochs, validation_split = validation_split, batch_size = batch_size, use_multiprocessing = True)\n",
    "    return model, history\n",
    "\n",
    "def decode_preds(preds, uniques):\n",
    "    func = np.vectorize(chr)\n",
    "    chars = preds.argmax(axis = 2)\n",
    "    words = func(uniques[chars]).tolist()\n",
    "    words = np.array([''.join(word) for word in words], dtype = str)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2667/2667 [==============================] - 40s 14ms/step - loss: 0.5283 - accuracy: 0.8266 - val_loss: 0.3277 - val_accuracy: 0.8832\n",
      "Epoch 2/5\n",
      "2667/2667 [==============================] - 32s 12ms/step - loss: 0.2331 - accuracy: 0.9205 - val_loss: 0.1681 - val_accuracy: 0.9459\n",
      "Epoch 3/5\n",
      "2667/2667 [==============================] - 33s 12ms/step - loss: 0.1124 - accuracy: 0.9649 - val_loss: 0.0822 - val_accuracy: 0.9746\n",
      "Epoch 4/5\n",
      "2667/2667 [==============================] - 33s 12ms/step - loss: 0.0669 - accuracy: 0.9802 - val_loss: 0.0488 - val_accuracy: 0.9856\n",
      "Epoch 5/5\n",
      "2667/2667 [==============================] - 34s 13ms/step - loss: 0.0400 - accuracy: 0.9885 - val_loss: 0.0339 - val_accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "model, history = build_model(train_x, train_y, epochs = 5, validation_split = 0.2, learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 7s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word-by-word accuracy of the model in deciphering the given cipher is 89.19%\n"
     ]
    }
   ],
   "source": [
    "preds = decode_preds(predictions, uniques)\n",
    "true = decode_preds(test_y, uniques)\n",
    "acc = np.sum(preds == true) / true.shape[0]\n",
    "print(f'The word-by-word accuracy of the model in deciphering the given cipher is {(acc * 100):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'epiphyllous'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'epiphyllous'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba14182f2addf8401c8bd3cfab2a612ede3e43383081db6c97cf7288d8247d9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
